# Import necessary modules and libraries
# langchain: Used for creating and managing AI agents and tools
# openvino_genai: Used for initializing and using OpenVINO's LLM pipeline
# smtplib, email.mime: Used for sending emails via SMTP
# dotenv: Used for loading environment variables from a .env file
# os: Provides functions to interact with the operating system
from langchain.agents import initialize_agent, Tool, AgentExecutor, AgentType
from langchain.tools import StructuredTool
from langchain.llms.base import LLM
from langchain.memory import ConversationBufferMemory
import openvino_genai as ov_genai
from typing import List, Optional
from pydantic import BaseModel
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from dotenv import load_dotenv
import os

from agent_tool.weather import get_weather_by_city
from agent_tool.sendEmail import send_email
from agent_tool.sendEmail import SendEmailInput
from agent_tool.searchWikipedia import search_wikipedia
# Load environment variables from a .env file
# This is used to securely store sensitive information like passwords
load_dotenv()
# Initialize OpenVINO LLM pipeline
# The model_path specifies the pre-trained model to use
# The device parameter specifies the hardware (e.g., NPU) for running the model
model_path = 'Phi-35_mini_instruct_refined'
pipe = ov_genai.LLMPipeline(model_path, device='NPU')
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
# Define a custom LLM class for OpenVINO
# This class integrates OpenVINO's LLM pipeline with LangChain's LLM interface
class OpenVINO_LLM(LLM):
    def __init__(self, pipeline: ov_genai.LLMPipeline, **kwargs):
        super().__init__(**kwargs)
        self._pipeline = pipeline

    # Property to access the pipeline
    @property
    def pipeline(self) -> ov_genai.LLMPipeline:
        return self._pipeline

    # Property to define the LLM type
    @property
    def _llm_type(self) -> str:
        return "openvino_genai"

    # Method to process a prompt and generate a response
    

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        #max_context_length = 1024
        #if len(prompt) > max_context_length:
        #    prompt = prompt[:max_context_length]
        response = []
        # Capture the output generated by the pipeline
        def capture_output(subword):
            response.append(subword)
            return False
        self.pipeline.generate(prompt, streamer=capture_output)
        return "".join(response)
    
# Instantiate the OpenVINO LLM
openvino_llm = OpenVINO_LLM(pipeline=pipe)


# Function to open the calculator application
# Returns a success or error message
def open_calculator() -> str:
    try:
        #print("DEBUG: open_calculator() 被觸發！")  # Debug message
        os.startfile("calc.exe")  # Open the calculator application
        return "Calculator opened successfully! STOP"
    except FileNotFoundError:
        return "Calculator application not found. STOP"
    except Exception as e:
        return f"An error occurred: {str(e)}. STOP"

def open_paint() -> str:
    try:
        os.startfile("mspaint.exe")  # Open the Paint application
        return "Paint opened successfully!"
    except FileNotFoundError:
        return "Paint application not found."


# Define the tools for the agent
# send_email_tool: Sends an email using the send_email function
# open_calculator_tool: Opens the calculator application
send_email_tool = StructuredTool(
    name="SendEmail",
    func=send_email,
    description=(
        "Send an email. Input should be a dictionary with 'recipient', 'subject', and 'body'."
        "Once the email is sent successfully, STOP further processing immediately. "
    ),
    args_schema=SendEmailInput  # Specify the input schema
)

open_calculator_tool = Tool(
    name="Open Calculator",
    func=lambda _: open_calculator(),  # Ensure the tool can be executed
    #func=open_calculator,  # Ensure the tool can be executed
    description="ALWAYS use this tool IMMEDIATELY when the user asks to open the calculator in Windows system. After using this tool, STOP further processing and do not provide any additional responses.",
    # Force to run open caclulator tool and response always the same
    return_direct=True,  # Ensure the tool returns the result directly
    
    #description=(
    #    "Use this tool to open the calculator in the Windows system. "
    #    "Once the calculator is opened, STOP further processing immediately. "
    #    "Do not attempt to observe or reason further after using this tool."
    #)
)

open_paint_tool = Tool(
    name="Open Paint",
    func=lambda _: open_paint(),  # Ensure the tool can be executed
    #func=open_paint,  # Ensure the tool can be executed
    description="ALWAYS use this tool IMMEDIATELY when the user asks to open the Paint."
)

# 定義查詢天氣的工具
weather_tool = Tool(
    name="GetWeather",
    func=lambda city: get_weather_by_city(city),
    description="Use this tool to get the current weather for a specific city. Input should be the city name as a string.STOP further processing and do not provide any additional responses."
)

# Define the search Wikipedia tool
search_wikipedia_tool = Tool(
    name="Search Wikipedia",
    func=lambda query: search_wikipedia(query),
    description= (
        "Use this tool to search for a summary of a topic on Wikipedia. "
        "Input should be a string representing the topic to search for. "
        "The tool will return a brief summary of the topic."
        "STOP further processing and do not provide any additional responses."
    )

)
# Initialize the agent with the defined tools and OpenVINO LLM
# The agent is configured to use a structured chat approach
#tools = [send_email_tool, open_calculator_tool, open_paint_tool,weather_tool,search_wikipedia_tool]
#tools = [send_email_tool,open_calculator_tool,weather_tool,search_wikipedia_tool]
tools = [send_email_tool,open_calculator_tool, weather_tool,search_wikipedia_tool]
agent = initialize_agent(
    tools=tools,
    llm=openvino_llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,  # Enable verbose logging for debugging
    #max_iterations=5,  # Maximum number of iterations for the agent to process input
    #memory=memory,  # Use the conversation memory to maintain context
)

# Create an AgentExecutor to manage the agent and tools
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    #system_message="You are an AI assistant that uses tools. Execute the requested tool and provide the result immediately. Do not re-execute the same tool unnecessarily.",
    system_message = (
        "You are an AI assistant that uses tools. "
        "When a tool like 'SendEmail', 'Open Calculator', 'GetWeather' is executed, "
        "STOP further processing immediately and do not provide any additional responses. "
        "If the tool returns a message containing 'STOP', treat it as a signal to stop processing."
        "Do not attempt to execute any other tools or reason after receiving a 'STOP' signal."
    ),
    return_intermediate_steps=True,  # Return intermediate steps for debugging
    handle_parse_errors=True,  # If the response do not have a perfect result, it will show the error or
)
